{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy.wcs import WCS\n",
    "import matplotlib.animation as animation\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.aperture import CircularAperture, CircularAnnulus, aperture_photometry\n",
    "from photutils.background import Background2D, MedianBackground\n",
    "from photutils import DAOStarFinder\n",
    "from astropy.stats import mad_std\n",
    "from cosmos_helper_funcs import get_stacks, label_plot, cosmos_gain_dict, get_mean_images\n",
    "from astroquery.vizier import Vizier\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "from astropy.table import Table\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.io\n",
    "\n",
    "data_folder = '/Volumes/Extreme SSD/COSMOS data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '/Users/layden/Library/CloudStorage/Box-Box/Scientific CMOS - MKI ONLY (might contain EAR; ITAR)/Teledyne_COSMOS/Analysis Images/On Sky Data/calibrated_with_WCS.fits'\n",
    "data_file_2 = '/Users/layden/Library/CloudStorage/Box-Box/Scientific CMOS - MKI ONLY (might contain EAR; ITAR)/Teledyne_COSMOS/Analysis Images/On Sky Data/calibrated_CMS_RS_M38_r_filter1000ms_-25C 2024 November 01 08_20_10.fits'\n",
    "# data_file_2 = '/Users/layden/Library/CloudStorage/Box-Box/Scientific CMOS - MKI ONLY (might contain EAR; ITAR)/Teledyne_COSMOS/Analysis Images/On Sky Data/Uncalibrated/CMS_RS_M38_r_filter1000ms_-25C 2024 November 01 08_20_10.fits'\n",
    "dark_file = '/Users/layden/Library/CloudStorage/Box-Box/Scientific CMOS - MKI ONLY (might contain EAR; ITAR)/Teledyne_COSMOS/Analysis Images/On Sky Data/calibrated_CMS_RS_dark_1000ms_-25C 2024 October 31 21_36_05.fits'\n",
    "defect_pix_file_1 = '/Users/layden/Library/CloudStorage/Box-Box/Scientific CMOS - MKI ONLY (might contain EAR; ITAR)/Teledyne_COSMOS/Final Data Products/Calibration/Defect Maps/defect_map_calibration_CMS.fits'\n",
    "defect_pix_file_2 = '/Users/layden/Library/CloudStorage/Box-Box/Scientific CMOS - MKI ONLY (might contain EAR; ITAR)/Teledyne_COSMOS/Final Data Products/Calibration/Defect Maps/defect_map_dark_CMS.fits'\n",
    "hdu = fits.open(data_file)[0]\n",
    "wcs = WCS(hdu.header)\n",
    "data = hdu.data\n",
    "hdu2 = fits.open(data_file_2)[0]\n",
    "data2 = hdu2.data.astype(int)\n",
    "data = data2[1]\n",
    "# data = np.mean(data2, axis=0)\n",
    "dark_data = fits.getdata(dark_file).astype(int)\n",
    "dark_data_avg = np.mean(dark_data, axis=0)\n",
    "data = data - dark_data_avg\n",
    "bkg_sigma = mad_std(data)  # Background standard deviation\n",
    "defect_data_1 = fits.getdata(defect_pix_file_1)\n",
    "defect_data_2 = fits.getdata(defect_pix_file_2)\n",
    "defect_data_1[1257] = np.NaN\n",
    "defect_data_1[6451] = np.NaN\n",
    "defect_data_1[:,994] = np.NaN\n",
    "data = data * defect_data_1 * defect_data_2\n",
    "# Print the pixel scale\n",
    "print(wcs.pixel_scale_matrix * 3600)\n",
    "print(np.nanmedian(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the flat field correction\n",
    "flat_field_file = '/Users/layden/Library/CloudStorage/Box-Box/Scientific CMOS - MKI ONLY (might contain EAR; ITAR)/Teledyne_COSMOS/Analysis Images/On Sky Data/calibrated_CMS_RS_SkyFlats_1000ms_-25_r_filter 2024 October 31 21_19_20.fits'\n",
    "flat_field_data = fits.getdata(flat_field_file)\n",
    "flat_field_mean_img = np.mean(flat_field_data, axis=0)\n",
    "flat_field = (flat_field_mean_img - dark_data_avg) * defect_data_1 * defect_data_2\n",
    "# Divide by the upper 0.5 percentile to normalize the flat field without outliers skewing the result\n",
    "flat_field = flat_field / np.nanpercentile(flat_field, 99.5)\n",
    "data = data / flat_field\n",
    "# Turn NaNs into background value\n",
    "print(np.nanmedian(data))\n",
    "data = np.nan_to_num(data, nan=np.nanmedian(data))\n",
    "data = data + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daofind = DAOStarFinder(fwhm=3.0, threshold=100)\n",
    "sources = daofind(data)\n",
    "positions = [(x, y) for x, y in zip(sources['xcentroid'], sources['ycentroid'])]\n",
    "# Define aperture and annulus radii\n",
    "aperture_radius = 6  # pixels\n",
    "annulus_inner_radius = 18  # pixels\n",
    "annulus_outer_radius = 21  # pixels\n",
    "\n",
    "# Create apertures and annuli\n",
    "apertures = CircularAperture(positions, r=aperture_radius)\n",
    "annuli = CircularAnnulus(positions, r_in=annulus_inner_radius, r_out=annulus_outer_radius)\n",
    "\n",
    "# Perform aperture photometry on the image\n",
    "aperture_photometry_result = aperture_photometry(data, apertures)\n",
    "\n",
    "# Perform background photometry using annuli\n",
    "annulus_masks = annuli.to_mask(method='center')\n",
    "background_medians = []\n",
    "\n",
    "for mask in annulus_masks:\n",
    "    annulus_data = mask.multiply(data)\n",
    "    annulus_data_1d = annulus_data[mask.data > 0]  # Exclude zero-padding\n",
    "    # Ignore warning for clipping NaNs\n",
    "    _, median_background, _ = sigma_clipped_stats(annulus_data_1d, sigma=3.0)\n",
    "    background_medians.append(median_background)\n",
    "\n",
    "# Subtract background and calculate final flux\n",
    "aperture_sums = aperture_photometry_result['aperture_sum']\n",
    "background_sums = np.array(background_medians) * apertures.area\n",
    "final_fluxes = aperture_sums - background_sums\n",
    "\n",
    "# Add results to the photometry table\n",
    "aperture_photometry_result['RA'], aperture_photometry_result['DEC'] = wcs.all_pix2world(aperture_photometry_result['xcenter'], aperture_photometry_result['ycenter'], 0)\n",
    "aperture_photometry_result['background'] = background_medians\n",
    "aperture_photometry_result['final_flux'] = final_fluxes\n",
    "# aperture_photometry_result = aperture_photometry_result[~np.isnan(aperture_photometry_result['final_flux'])]\n",
    "aperture_photometry_result.sort('final_flux')\n",
    "# Only keep those with positive final flux\n",
    "aperture_photometry_result = aperture_photometry_result[aperture_photometry_result['final_flux'] > 0]\n",
    "# Exclude stars within 50 pixels of the edge\n",
    "aperture_photometry_result = aperture_photometry_result[(aperture_photometry_result['xcenter'] / u.pix > 50)]\n",
    "aperture_photometry_result = aperture_photometry_result[(aperture_photometry_result['ycenter'] / u.pix > 50)]\n",
    "aperture_photometry_result = aperture_photometry_result[(aperture_photometry_result['xcenter'] / u.pix < 8070)]\n",
    "aperture_photometry_result = aperture_photometry_result[(aperture_photometry_result['ycenter'] / u.pix < 8070)]\n",
    "\n",
    "# Print the results\n",
    "print(aperture_photometry_result)\n",
    "\n",
    "# # Exclude stars that are within 30 pixels of each other\n",
    "# # Find distance to nearest star for each star\n",
    "# distances = np.zeros(len(aperture_photometry_result))\n",
    "# for i, entry in enumerate(aperture_photometry_result):\n",
    "#     xpos = entry['xcenter']\n",
    "#     ypos = entry['ycenter']\n",
    "#     min_dist = 100\n",
    "#     for j, entry2 in enumerate(aperture_photometry_result):\n",
    "#         if i == j:\n",
    "#             continue\n",
    "#         xpos2 = entry2['xcenter']\n",
    "#         ypos2 = entry2['ycenter']\n",
    "#         distance = np.sqrt((xpos - xpos2) ** 2 + (ypos - ypos2) ** 2) / u.pix\n",
    "#         if distance < min_dist:\n",
    "#             min_dist = distance\n",
    "#     distances[i] = min_dist\n",
    "#     print(distances[i])\n",
    "\n",
    "# apertures = CircularAperture(positions, r=5)  # 5-pixel radius aperture\n",
    "# phot_table = aperture_photometry(data, apertures)\n",
    "# phot_table['RA'], phot_table['DEC'] = wcs.all_pix2world(phot_table['xcenter'], phot_table['ycenter'], 0)\n",
    "# # Drop any stars with NaNs\n",
    "# phot_table = phot_table[~np.isnan(phot_table['aperture_sum'])]\n",
    "# print(phot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_center = SkyCoord.from_pixel(hdu.data.shape[1] / 2, hdu.data.shape[0] / 2, wcs)\n",
    "print(f\"Image center (RA, Dec): {image_center.ra.deg}, {image_center.dec.deg}\")\n",
    "# Use the center position for querying the catalog\n",
    "radius = 60 * u.arcmin  # Set your desired search radius\n",
    "\n",
    "# Query the star catalog\n",
    "Vizier.ROW_LIMIT = -1  # Remove row limit for large queries\n",
    "result = Vizier.query_region(\n",
    "    image_center, radius=radius, catalog='I/345/gaia2')[0]\n",
    "\n",
    "# Only keep objects with Gmag below 17.5\n",
    "result = result[result['Gmag'] < 17.5]\n",
    "# Convert every star in result to an x and y position on COSMOS\n",
    "coords = SkyCoord(result['RA_ICRS'], result['DE_ICRS'], unit='deg')\n",
    "x, y = wcs.all_world2pix(coords.ra, coords.dec, 0)\n",
    "result['x'] = x\n",
    "result['y'] = y\n",
    "# Only keep stars that are within the image\n",
    "result = result[(result['x'] >= 0) & (result['x'] < hdu.data.shape[1]) &\n",
    "                (result['y'] >= 0) & (result['y'] < hdu.data.shape[0])]\n",
    "# print(result['RA_ICRS', 'DE_ICRS', 'Gmag', 'x', 'y'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the proximity to the nearest star for each star in the catalog\n",
    "distances = np.zeros(len(result))\n",
    "distance_cutoff = 35\n",
    "for i, entry in enumerate(result):\n",
    "    xpos = entry['x']\n",
    "    ypos = entry['y']\n",
    "    min_dist = 100\n",
    "    for j, entry2 in enumerate(result):\n",
    "        if i == j:\n",
    "            continue\n",
    "        xpos2 = entry2['x']\n",
    "        ypos2 = entry2['y']\n",
    "        distance = np.sqrt((xpos - xpos2) ** 2 + (ypos - ypos2) ** 2)\n",
    "        if distance < min_dist:\n",
    "            min_dist = distance\n",
    "        if min_dist < distance_cutoff:\n",
    "            break\n",
    "    distances[i] = min_dist\n",
    "    print(i, distances[i])\n",
    "# Make distances a row in the table\n",
    "result['distance'] = distances\n",
    "# Remove stars that are too close to each other\n",
    "result = result[result['distance'] > distance_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Gaia\n",
    "# Match the photometry table with the star catalog\n",
    "coords = SkyCoord(ra=aperture_photometry_result['RA'] * u.deg, dec=aperture_photometry_result['DEC'] * u.deg)\n",
    "coords_catalog = SkyCoord(ra=result['RA_ICRS'], dec=result['DE_ICRS'])\n",
    "idx, d2d, d3d = match_coordinates_sky(coords, coords_catalog)\n",
    "# Filter matches with a threshold (e.g., 1 arcsecond)\n",
    "threshold = 2 * u.arcsec\n",
    "matched = d2d < threshold\n",
    "matched_catalog = Table({\n",
    "                    'RA': result['RA_ICRS'][idx[matched]],\n",
    "                    'Dec': result['DE_ICRS'][idx[matched]],\n",
    "                    'Gmag': result['Gmag'][idx[matched]],\n",
    "                    'e_Gmag': result['e_Gmag'][idx[matched]],\n",
    "                    'xcentroid': aperture_photometry_result['xcenter'][matched],\n",
    "                    'ycentroid': aperture_photometry_result['ycenter'][matched],\n",
    "                    'ra_image': aperture_photometry_result['RA'][matched],\n",
    "                    'dec_image': aperture_photometry_result['DEC'][matched],\n",
    "                    'aperture_sum': aperture_photometry_result['aperture_sum'][matched],\n",
    "                    'meas_flux': aperture_photometry_result['final_flux'][matched],\n",
    "                    'separation': d2d[matched].arcsec,\n",
    "                    'gaia_id': result['Source'][idx[matched]],\n",
    "                    'BP-RP': result['BP-RP'][idx[matched]]\n",
    "                    })\n",
    "matched_catalog.sort('meas_flux')\n",
    "\n",
    "print(matched_catalog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelengths = np.arange(336, 1022, 2)\n",
    "sloan_r_file = '/Users/layden/Downloads/SLOAN.SDSS.rprime_filter.xml'\n",
    "sloan_r = Table.read(sloan_r_file)\n",
    "# Convert to numpy array\n",
    "sloan_r = sloan_r.to_pandas()\n",
    "sloan_r['Wavelength'] /= 10\n",
    "# Interpolate the filter to find values at elements of wavelengths\n",
    "sloan_r_interp = np.interp(wavelengths, sloan_r['Wavelength'], sloan_r['Transmission'])\n",
    "cosmos_arr = np.genfromtxt('/Users/layden/Documents/Image-Sensor-Testing/data/cosmos_qe_datasheet.csv', delimiter=',')\n",
    "cosmos_arr_interp = np.interp(wavelengths, cosmos_arr[:, 0], cosmos_arr[:, 1])\n",
    "plt.plot(wavelengths, sloan_r_interp)\n",
    "plt.plot(wavelengths, cosmos_arr_interp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Gaia spectra to calculate expected flux with our filter and QE\n",
    "filename = '/Users/layden/Downloads/XpSampledMeanSpectrum_020748-020984.csv'\n",
    "import pandas as pd\n",
    "# Load csv into pandas dataframe\n",
    "col_names = ['source_id', 'solution_id', 'ra', 'dec', 'flux', 'flux_err']\n",
    "# Use 100 rows\n",
    "df = pd.read_csv(filename, skiprows=64, names=col_names)\n",
    "source_ids = df['source_id'].to_numpy()\n",
    "# Want to convert Gaia fluxes to phot/m^2/nm/s. They're currently in W/m^2/nm\n",
    "h = 6.62607015e-34  # Planck's constant in J s\n",
    "c = 299792458  # Speed of light in m/s\n",
    "# Create column in matched_catalog for fluxes\n",
    "matched_catalog['gaia_phot_flux'] = np.zeros(len(matched_catalog), dtype=object)\n",
    "matched_catalog['gaia_phot_flux_err'] = np.zeros(len(matched_catalog), dtype=object)\n",
    "for i, star in enumerate(matched_catalog):\n",
    "    idx = np.where(source_ids == star['gaia_id'])[0]\n",
    "    if len(idx) > 0:\n",
    "        fluxes = df['flux'][idx[0]]\n",
    "        flux_errs = df['flux_err'][idx[0]]\n",
    "        # Convert fluxes string into numpy array. First remove brackets\n",
    "        fluxes = fluxes[1:-1].split(',')\n",
    "        flux_errs = flux_errs[1:-1].split(',')\n",
    "        fluxes = np.array(fluxes, dtype=float)\n",
    "        flux_errs = np.array(flux_errs, dtype=float)\n",
    "        avg_flux_err = np.mean(flux_errs / fluxes)\n",
    "        phot_fluxes = fluxes / (h * c / (wavelengths * 1e-9))\n",
    "        phot_fluxes = phot_fluxes * sloan_r_interp * cosmos_arr_interp\n",
    "        tot_phot_flux = np.sum(phot_fluxes)\n",
    "        matched_catalog['gaia_phot_flux'][i] = tot_phot_flux\n",
    "        matched_catalog['gaia_phot_flux_err'][i] = abs(avg_flux_err)\n",
    "# Exclude stars with no Gaia spectra\n",
    "matched_catalog = matched_catalog[matched_catalog['gaia_phot_flux'] > 0]\n",
    "matched_catalog = matched_catalog[matched_catalog['gaia_phot_flux_err'] < 0.1]\n",
    "print(matched_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vs_multiplier = np.genfromtxt('mean_vs_multiplier.csv', delimiter=',', skip_header=1)\n",
    "mean_vals = mean_vs_multiplier[:, 0]\n",
    "multiplier_vals = mean_vs_multiplier[:, 1]\n",
    "gain = 4.3\n",
    "raw_read_noise = 0.8\n",
    "raw_bkg_level = 1.0 # e-\n",
    "# Use mean image to figure out how much shot noise should be multiplied to account\n",
    "# for effective noise due to nonlinearity\n",
    "matched_catalog_positions = [(x, y) for x, y in zip(matched_catalog['xcentroid'], matched_catalog['ycentroid'])]\n",
    "shot_noise_vals = np.zeros(len(matched_catalog_positions))\n",
    "read_noise_vals = np.zeros(len(matched_catalog_positions))\n",
    "bkg_noise_vals = np.zeros(len(matched_catalog_positions))\n",
    "for i, position in enumerate(matched_catalog_positions):\n",
    "    star_subarray = data[int(position[1]) - 10:int(position[1]) + 10,\n",
    "                         int(position[0]) - 10:int(position[0]) + 10] - 100\n",
    "    multiplier_subarray = np.interp(star_subarray, mean_vals, multiplier_vals)\n",
    "    var_shotnoise_subarray = gain * star_subarray * multiplier_subarray\n",
    "    var_readnoise_subarray = (raw_read_noise * gain * multiplier_subarray) ** 2\n",
    "    var_bkgnoise_subarray = (raw_bkg_level * gain * multiplier_subarray) ** 2\n",
    "    aperture = CircularAperture([10,10], r=aperture_radius)\n",
    "    shot_var = aperture_photometry(var_shotnoise_subarray, aperture)['aperture_sum'][0]\n",
    "    read_var = aperture_photometry(var_readnoise_subarray, aperture)['aperture_sum'][0]\n",
    "    bkg_var = aperture_photometry(var_bkgnoise_subarray, aperture)['aperture_sum'][0]\n",
    "    shot_noise_vals[i] = np.sqrt(shot_var)\n",
    "    read_noise_vals[i] = np.sqrt(read_var)\n",
    "    bkg_noise_vals[i] = np.sqrt(bkg_var)\n",
    "# PTC noise vals are the standard deviation in e- expected corresponding to stars in the matched catalog\n",
    "# Get rid of outliers in the PTC noise values and gaia flux values\n",
    "start_val = 0\n",
    "# plt.plot(matched_catalog['gaia_phot_flux'][start_val:], ptc_noise_vals[start_val:], 'o')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.plot(plot_points, ptc_noise_vals_interp, '-')\n",
    "indices_to_exclude = [694]\n",
    "# indices_to_exclude = [694, 730, 731, 735, 736, 737, 738, 739, 742, 743, 744, 745]\n",
    "clean_shot_noise_vals = np.delete(shot_noise_vals, indices_to_exclude)\n",
    "clean_read_noise_vals = np.delete(read_noise_vals, indices_to_exclude)\n",
    "clean_bkg_noise_vals = np.delete(bkg_noise_vals, indices_to_exclude)\n",
    "clean_gaia_phot_flux = np.delete(matched_catalog['gaia_phot_flux'].data.astype(np.float64), indices_to_exclude)\n",
    "clean_e_flux_vals = np.delete(matched_catalog['meas_flux'].data.astype(np.float64) / gain, indices_to_exclude)\n",
    "plt.plot(clean_e_flux_vals, clean_shot_noise_vals, '-')\n",
    "plt.plot(clean_e_flux_vals, clean_read_noise_vals, '-')\n",
    "plt.plot(clean_e_flux_vals, clean_bkg_noise_vals, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Increase resolution for plots\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "gain = 4.3\n",
    "matched_catalog['meas_e_flux'] = matched_catalog['meas_flux'] / gain\n",
    "# Convert Gmag to flux\n",
    "matched_catalog['flux'] = 10 ** (-0.4 * (matched_catalog['gaia_phot_flux'] - 25.688365))\n",
    "# plt.plot(matched_catalog['Gmag'], line(matched_catalog['Gmag'], *popt2), color='red')\n",
    "# plt.ylim(0, 0.002)\n",
    "# plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4.5))\n",
    "plot_points = np.linspace(np.min(matched_catalog['gaia_phot_flux']), np.max(matched_catalog['gaia_phot_flux']), 1000)\n",
    "# Increase font size\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "popt, _ = curve_fit(lambda x, a, b: a * x + b, matched_catalog['gaia_phot_flux'], matched_catalog['meas_e_flux'])\n",
    "# ax.plot(matched_catalog['gaia_phot_flux'], matched_catalog['meas_e_flux'], 'o', markersize=2, label='Measured Data')\n",
    "# Make label for best fit line have slope\n",
    "ax.plot(plot_points, np.polyval(popt, plot_points), 'r--', label=f'Best fit line, slope = {popt[0]:.2f} $m^2$')\n",
    "ax.set_xlabel(r'Expected signal flux ($e^-/m^2/s$)', fontsize=14)\n",
    "ax.set_ylabel(r'Measured signal ($e^-/s$)', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "read_noise_vals_interp = np.interp(matched_catalog['gaia_phot_flux'].data.astype(np.float64), clean_gaia_phot_flux, clean_read_noise_vals / gain)\n",
    "shot_noise_vals_interp = np.interp(matched_catalog['gaia_phot_flux'].data.astype(np.float64), clean_gaia_phot_flux, clean_shot_noise_vals / gain)\n",
    "bkg_noise_vals_interp = np.interp(matched_catalog['gaia_phot_flux'].data.astype(np.float64), clean_gaia_phot_flux, clean_bkg_noise_vals / gain)\n",
    "scint_noise_vals = np.polyval(popt, matched_catalog['gaia_phot_flux'].data.astype(np.float64)) * 0.005\n",
    "tot_noise_vals = np.sqrt(read_noise_vals_interp ** 2 + shot_noise_vals_interp ** 2 + bkg_noise_vals_interp ** 2 + scint_noise_vals ** 2)\n",
    "# tot_noise_vals_percent = tot_noise_vals / func(x, *popt) * 100\n",
    "# tot_noise_vals_percent_2 = tot_noise_vals_2 / func(x, *popt) * 100\n",
    "# ax.plot(plot_points, np.polyval(popt, plot_points) + tot_noise_vals, 'k:', label='Expected Noise')\n",
    "# ax.plot(plot_points, np.polyval(popt, plot_points) - tot_noise_vals, 'k:')\n",
    "ax.errorbar(matched_catalog['gaia_phot_flux'], matched_catalog['meas_e_flux'],\n",
    "            xerr=matched_catalog['gaia_phot_flux_err'] * matched_catalog['gaia_phot_flux'],\n",
    "            yerr=tot_noise_vals, fmt='o', markersize=0.5, linewidth=0.2, label='Measured data', zorder=0)\n",
    "plt.legend(fontsize=11, loc='lower right')\n",
    "plt.xlim(3e3, 1e6)\n",
    "plt.ylim(4e2, 1e5)\n",
    "# Increase padding between subplots\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the 10 brightest stars. Make a 50x50 cutout around those stars and fit to a 2D Gaussian. Report the FWHM\n",
    "brightest_stars = matched_catalog[-100:]\n",
    "cutouts = []\n",
    "for star in brightest_stars:\n",
    "    xpos = star['xcentroid']\n",
    "    ypos = star['ycentroid']\n",
    "    cutout = data[int(ypos) - 25:int(ypos) + 25, int(xpos) - 25:int(xpos) + 25]\n",
    "    cutouts.append(cutout)\n",
    "\n",
    "def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "    \"\"\"\n",
    "    2D Gaussian function.\n",
    "    \"\"\"\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)\n",
    "    a = (np.cos(theta)**2) / (2*sigma_x**2) + (np.sin(theta)**2) / (2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta)) / (4*sigma_x**2) + (np.sin(2*theta)) / (4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2) / (2*sigma_x**2) + (np.cos(theta)**2) / (2*sigma_y**2)\n",
    "    g = offset + amplitude * np.exp(-(a * (x - xo)**2 + 2*b * (x - xo) * (y - yo) + c * (y - yo)**2))\n",
    "    return g.ravel()\n",
    "\n",
    "def fit_gaussian_and_get_fwhm(cutout):\n",
    "    \"\"\"\n",
    "    Fit a 2D Gaussian to a cutout and calculate the FWHM.\n",
    "    \"\"\"\n",
    "    # Create coordinate grid\n",
    "    x = np.arange(cutout.shape[1])\n",
    "    y = np.arange(cutout.shape[0])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Initial guesses for fitting\n",
    "    amplitude = cutout.max() - cutout.min()\n",
    "    xo, yo = np.unravel_index(np.argmax(cutout), cutout.shape)\n",
    "    sigma_x = sigma_y = 3.0  # Initial guess for sigma\n",
    "    theta = 0.0  # Assume no rotation\n",
    "    offset = cutout.min()\n",
    "    initial_guess = (amplitude, xo, yo, sigma_x, sigma_y, theta, offset)\n",
    "    \n",
    "    try:\n",
    "        # Fit the 2D Gaussian\n",
    "        popt, _ = curve_fit(gaussian_2d, (x, y), cutout.ravel(), p0=initial_guess)\n",
    "    except RuntimeError:\n",
    "        return None  # Fit failed\n",
    "\n",
    "    # Extract fitted parameters\n",
    "    _, _, _, sigma_x, sigma_y, _, _ = popt\n",
    "\n",
    "    # Calculate FWHM: FWHM = 2.355 * sigma\n",
    "    fwhm_x = 2.355 * sigma_x\n",
    "    fwhm_y = 2.355 * sigma_y\n",
    "    if abs(fwhm_x) > 10 or abs(fwhm_y) > 10:\n",
    "        return None\n",
    "\n",
    "    return fwhm_x, fwhm_y\n",
    "\n",
    "fwhm_results = np.zeros((len(cutouts), 2))\n",
    "for i, cutout in enumerate(cutouts):\n",
    "    result = fit_gaussian_and_get_fwhm(cutout)\n",
    "    if result:\n",
    "        fwhm_results[i] = result\n",
    "    else:\n",
    "        fwhm_results[i] = (None, None)\n",
    "\n",
    "plt.plot(fwhm_results[:,0], 'o')\n",
    "plt.plot(fwhm_results[:,1], 'o')\n",
    "plt.show()\n",
    "avg_fwhm = np.nanmean(fwhm_results) * wcs.pixel_scale_matrix[0, 0] * 3600\n",
    "std_fwhm = np.nanstd(fwhm_results) * wcs.pixel_scale_matrix[0, 0] * 3600\n",
    "print(\"FWHM: \", format(avg_fwhm, '3.2f'), \"plus/minus\", format(std_fwhm, '3.3f'), \"arcsec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_values(arr, del_x, del_y):\n",
    "    '''Shift values in an array by a specified discrete displacement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array-like\n",
    "        The intensity grid.\n",
    "    del_x : int\n",
    "        The x displacement, in subpixels.\n",
    "    del_y : int\n",
    "        The y displacement, in subpixels.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    new_arr : array-like\n",
    "        The shifted array.\n",
    "    '''\n",
    "    m, n = arr.shape\n",
    "    new_arr = np.zeros_like(arr)\n",
    "    # print(abs(del_x) > m, abs(del_y) > n)\n",
    "    new_arr[max(del_y, 0):m+min(del_y, 0), max(del_x, 0):n+min(del_x, 0)] = \\\n",
    "        arr[-min(del_y, 0):m-max(del_y, 0), -min(del_x, 0):n-max(del_x, 0)]\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = '/Users/layden/Library/CloudStorage/Box-Box/Scientific CMOS - MKI ONLY (might contain EAR; ITAR)/Teledyne_COSMOS/Analysis Images/On Sky Data/Light Curve'\n",
    "stacks = get_stacks(source_folder)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate imagestacks into a single array\n",
    "imagestack = np.concatenate([stack['imagestack'] for stack in stacks], axis=0)\n",
    "print(0)\n",
    "imagestack = imagestack.astype(np.float32)\n",
    "print(1)\n",
    "# Remove defect pixels\n",
    "imagestack = imagestack * defect_data_1 * defect_data_2\n",
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarray_size = 200\n",
    "start_x, start_y = 1150, 3699\n",
    "shift_x, shift_y = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cube = np.zeros((len(imagestack), 200, 200))\n",
    "shifts = np.zeros((len(imagestack), 2))\n",
    "for i in range(len(imagestack)):\n",
    "    print('Starting Frame', i)\n",
    "    frame = imagestack[i]\n",
    "    start_x -= shift_x\n",
    "    start_y -= shift_y\n",
    "    # Clear previous stack_data from memory\n",
    "    # subarray_dark = dark_data_avg[start_y - subarray_size // 2:start_y + subarray_size // 2,\n",
    "    #                            start_x - subarray_size // 2:start_x + subarray_size // 2]\n",
    "    # subarray_flat = flat_field[start_y - subarray_size // 2:start_y + subarray_size // 2,\n",
    "    #                            start_x - subarray_size // 2:start_x + subarray_size // 2]\n",
    "    subarray_data = frame[start_y - subarray_size // 2:start_y + subarray_size // 2,\n",
    "                        start_x - subarray_size // 2:start_x + subarray_size // 2]\n",
    "    subarray_data = np.nan_to_num(subarray_data, nan=106)\n",
    "    # subarray_data = (subarray_data - subarray_dark) / subarray_flat\n",
    "    max_index = np.argmax(subarray_data)\n",
    "    max_coords = np.unravel_index(max_index, subarray_data.shape)\n",
    "    \n",
    "    y, x = np.mgrid[:subarray_data.shape[0], :subarray_data.shape[1]]\n",
    "    initial_guess = models.Gaussian2D(amplitude=subarray_data.max(), x_mean=max_coords[1], y_mean=max_coords[0], x_stddev=3, y_stddev=3)\n",
    "    fitter = fitting.LevMarLSQFitter()\n",
    "    fitted_gaussian = fitter(initial_guess, x, y, subarray_data)\n",
    "    shift_x = np.rint(subarray_size / 2 - fitted_gaussian.x_mean.value).astype(int)\n",
    "    shift_y = np.rint(subarray_size / 2 - fitted_gaussian.y_mean.value).astype(int)\n",
    "    print(start_x, start_y, shift_x, shift_y)\n",
    "    shifted_frame = shift_values(subarray_data, shift_x, shift_y)\n",
    "    test_cube[i] = shifted_frame\n",
    "    shifts[i] = (start_x - shift_x, start_y - shift_y)\n",
    "shifts = (shifts - shifts[0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(imagestack)):\n",
    "    print('Starting Frame', i)\n",
    "    frame = (imagestack[i] - dark_data_avg) / flat_field + 100\n",
    "    frame = np.nan_to_num(frame, nan=np.nanmedian(frame))\n",
    "    frame = shift_values(frame, -shifts[i, 0], -shifts[i, 1])\n",
    "    imagestack[i] = frame\n",
    "mean_shifted_img = np.mean(imagestack, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# plt.imshow(mean_shifted_img, cmap='gray', norm='log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weird shifts happened, so we need to shift the positions\n",
    "# positions = [(x / u.pix - 16, y / u.pix + 3) for x, y in zip(aperture_photometry_result['xcenter'], aperture_photometry_result['ycenter'])]\n",
    "# positions = [(x - 16, y + 3) for x, y in zip(matched_catalog['xcentroid'], matched_catalog['ycentroid'])]\n",
    "daofind = DAOStarFinder(fwhm=5.0, threshold=100)\n",
    "sources_mean_img = daofind(mean_shifted_img)\n",
    "positions = [(x, y) for x, y in zip(sources_mean_img['xcentroid'], sources_mean_img['ycentroid'])]\n",
    "# Perform aperture photometry on each frame in the image cube\n",
    "light_curves_low = np.zeros((len(positions), len(imagestack)))\n",
    "gain = 4.3\n",
    "apertures = CircularAperture(positions, r=aperture_radius)\n",
    "annuli = CircularAnnulus(positions, r_in=annulus_inner_radius, r_out=annulus_outer_radius)\n",
    "# Perform background photometry using annuli\n",
    "annulus_masks = annuli.to_mask(method='center')\n",
    "\n",
    "for i, frame in enumerate(imagestack[:]):\n",
    "    print('Starting Frame', i)\n",
    "    img_frame = frame / gain\n",
    "    # Ignore warning for clipping NaNs\n",
    "    background_medians = []\n",
    "    for mask in annulus_masks:\n",
    "        annulus_data = mask.multiply(img_frame)\n",
    "        annulus_data_1d = annulus_data[mask.data > 0]  # Exclude zero-padding\n",
    "        # Ignore warning for clipping NaNs\n",
    "        _, median_background, _ = sigma_clipped_stats(annulus_data_1d, sigma=3.0)\n",
    "        background_medians.append(median_background)\n",
    "    aperture_phot_result = aperture_photometry(img_frame, apertures)\n",
    "    aperture_sums = aperture_phot_result['aperture_sum']\n",
    "    background_sums = np.array(background_medians) * apertures.area\n",
    "    final_fluxes = aperture_sums - background_sums\n",
    "    light_curves_low[:, i] = final_fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_vs_var = np.genfromtxt('mean_vs_var.csv', delimiter=',', skip_header=1)\n",
    "# mean_vals = mean_vs_var[:, 0]\n",
    "# var_vals = mean_vs_var[:, 1]\n",
    "# gain = 4.3\n",
    "# # Use mean image to figure out how much shot noise should be multiplied to account\n",
    "# # for effective noise due to nonlinearity\n",
    "# ptc_noise_vals_2 = np.zeros(len(positions))\n",
    "# for i, position in enumerate(positions):\n",
    "#     star_subarray = imagestack[0][int(position[1]) - 10:int(position[1]) + 10,\n",
    "#                          int(position[0]) - 10:int(position[0]) + 10] - 100\n",
    "#     shot_noise_subarray = np.interp(star_subarray / gain, mean_vals, var_vals)\n",
    "#     aperture = CircularAperture([10,10], r=aperture_radius)\n",
    "#     # phot_result = aperture_photometry(star_subarray, aperture)['aperture_sum'][0]\n",
    "#     noise_result = aperture_photometry(shot_noise_subarray, aperture)['aperture_sum'][0]\n",
    "#     ptc_noise_vals_2[i] = np.sqrt(noise_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Concatenate the light curves\n",
    "mean_bkg = 5 # ADU\n",
    "aper_size = np.pi * aperture_radius ** 2\n",
    "avg_brightnesses = np.mean(light_curves_low, axis=0) / np.mean(light_curves_low)\n",
    "light_curves_2 = light_curves_low / avg_brightnesses\n",
    "light_curve_means = np.mean(light_curves_2, axis=1)\n",
    "light_curve_errs = np.std(light_curves_2, axis=1) / light_curve_means\n",
    "plt.plot(light_curve_means, light_curve_errs, 'o', markersize=1, label='Measured data')\n",
    "mean_points = np.logspace(1, 6.3, 100)\n",
    "shot_noise_vals_interp = np.interp(mean_points, clean_e_flux_vals, clean_shot_noise_vals / gain)\n",
    "read_noise_vals_interp = np.interp(mean_points, clean_e_flux_vals, clean_read_noise_vals / gain)\n",
    "bkg_noise_vals_interp = np.interp(mean_points, clean_e_flux_vals, clean_bkg_noise_vals / gain)\n",
    "scint_noise_points = 0.005 * mean_points\n",
    "tot_noise_points = np.sqrt(shot_noise_vals_interp ** 2 + read_noise_vals_interp ** 2 + scint_noise_points ** 2 + bkg_noise_vals_interp ** 2) / mean_points\n",
    "naive_shotnoise_points = np.sqrt(mean_points)\n",
    "naive_readnoise_points = 0.8 * np.sqrt(aper_size)\n",
    "naive_bkg_points = np.sqrt(aper_size * mean_bkg)\n",
    "tot_naive_noise_points = np.sqrt(naive_shotnoise_points ** 2 + naive_readnoise_points ** 2 + naive_bkg_points ** 2 + scint_noise_points ** 2)\n",
    "plt.plot(mean_points, shot_noise_vals_interp / mean_points, 'g--', label='Effective shot noise')\n",
    "plt.plot(mean_points, read_noise_vals_interp / mean_points, 'r-.', label='Effective read noise')\n",
    "plt.plot(mean_points, bkg_noise_vals_interp / mean_points, 'b:', label='Effective background noise')\n",
    "plt.plot(mean_points, scint_noise_points / mean_points, ':', color='magenta', label='Scintillation noise')\n",
    "plt.plot(mean_points, tot_noise_points, 'k-', label='Total expected noise')\n",
    "plt.plot(mean_points, naive_shotnoise_points / mean_points, 'g--', label='Ideal shot noise', alpha=0.2)\n",
    "plt.plot(mean_points, naive_readnoise_points / mean_points, 'r-.', label='Ideal read noise', alpha=0.2)\n",
    "plt.plot(mean_points, naive_bkg_points / mean_points, 'b:', label='Ideal background noise', alpha=0.2)\n",
    "plt.plot(mean_points, tot_naive_noise_points / mean_points, 'k-', label='Ideal total noise', alpha=0.2)\n",
    "plt.legend(fontsize=10, ncol=2, loc='upper center')\n",
    "plt.xlabel(r'Measured mean signal ($e^-/s$)', fontsize=16)\n",
    "plt.ylabel('Noise-to-signal ratio', fontsize=16)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlim(3E2, 3E5)\n",
    "plt.ylim(0.003, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index of star with flux closest to 27109.6\n",
    "flux_target = 13581.15\n",
    "# flux_target = 27109.6\n",
    "idx = (np.abs(light_curve_means - flux_target)).argmin()\n",
    "print(\"Closest flux index: \", idx)\n",
    "print(light_curve_errs[idx])\n",
    "print(positions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(defect_data_2[6830:6840,3380:3390])\n",
    "print(defect_data_2[3380:3390,6830:6840])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just keep stuff with light_curve_mean above 1000\n",
    "light_curve_errs = light_curve_errs[light_curve_means > 1000]\n",
    "light_curves_2 = light_curves_2[light_curve_means > 1000]\n",
    "light_curve_means = light_curve_means[light_curve_means > 1000]\n",
    "max_index = np.argsort(light_curve_errs)[-6]\n",
    "print(light_curve_errs[max_index], light_curve_means[max_index])\n",
    "plt.plot(light_curves_2[max_index], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
